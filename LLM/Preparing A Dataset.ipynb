{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:28.059169Z",
     "start_time": "2025-06-23T01:44:27.991777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# 1) Where you downloaded wordnet/omw-1.4\n",
    "download_folder = r'C:\\Users\\Spyqi\\PycharmProjects\\Azure-ML-Practice\\LLM\\data'\n",
    "\n",
    "# 2) (Re)download—this is idempotent, so safe to run every time\n",
    "nltk.download('wordnet', download_dir=download_folder)\n",
    "nltk.download('omw-1.4',   download_dir=download_folder)\n",
    "\n",
    "# 3) Tell NLTK’s loader about that folder\n",
    "nltk.data.path.insert(0, download_folder)\n",
    "\n",
    "# 4) Now import and use WordNet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(wordnet.synsets('dog'))  # should work without LookupError\n"
   ],
   "id": "93679143f05a6511",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Spyqi\\PycharmProjects\\Azure-ML-\n",
      "[nltk_data]     Practice\\LLM\\data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Spyqi\\PycharmProjects\\Azure-ML-\n",
      "[nltk_data]     Practice\\LLM\\data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:28.648212Z",
     "start_time": "2025-06-23T01:44:28.620520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 1.) Import Dataset -- #\n",
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet    class  \\\n",
      "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
      "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
      "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
      "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
      "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
      "\n",
      "  sentiment_intensity class_intensity  labels  \n",
      "0                 low        fear_low       4  \n",
      "1                high    sadness_high       9  \n",
      "2              medium     fear_medium       5  \n",
      "3                high      anger_high       0  \n",
      "4                 low        fear_low       4  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:29.364282Z",
     "start_time": "2025-06-23T01:44:29.350223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Types of data\n",
    "#Labeled data: the dataset must include labeled data for supervised learning tasks such as sentiment analysis. In our case, the IMDB dataset is labeled with sentiment classes, such as \"positive\" or \"negative\" reviews.\n",
    "\n",
    "#Unlabeled data: unlabeled data can be used in unsupervised learning tasks or semi-supervised learning models. For this example, however, we will focus on labeled data for fine-tuning."
   ],
   "id": "63e3c7a536894cef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:29.878614Z",
     "start_time": "2025-06-23T01:44:29.850995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 2.) Clean The Text -- #\n",
    "import re # Import the `re` module for working with regular expressions\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert all text to lowercase for uniformity\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
    "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
    "    return text # Return the cleaned text\n",
    "\n",
    "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
    "# Apply the cleaning function to each row of the 'text' column\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
    "print(data['cleaned_text'].head())"
   ],
   "id": "a37c82b09152309a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    loved bethenny independence msg on wendywillia...\n",
      "1    mark_slifer actually maybe we were supposed to...\n",
      "2    i thought the nausea and headaches had passed ...\n",
      "3    anger resentment and hatred are the destroyer ...\n",
      "4      new tires amp an alarm system on my car fwm now\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:30.189188Z",
     "start_time": "2025-06-23T01:44:30.177117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 3.) Handle Missing Data -- #\n",
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum()) # Print the count of missing values for each column\n",
    "\n",
    "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
    "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
    "\n",
    "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('unknown') # Replace NaN values in 'cleaned_text' with 'unknown'"
   ],
   "id": "4d80663105c2aeb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "tweet                  0\n",
      "class                  0\n",
      "sentiment_intensity    0\n",
      "class_intensity        0\n",
      "labels                 0\n",
      "cleaned_text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:35.240888Z",
     "start_time": "2025-06-23T01:44:30.610795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 4.) Tokenization -- #\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
   ],
   "id": "de509fe9b1859f0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n",
      "         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n",
      "         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n",
      "          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n",
      "          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n",
      "          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n",
      "          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:44:36.189978Z",
     "start_time": "2025-06-23T01:44:35.257969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 5.) Augmentation -- #\n",
    "# -- Synonym Replacement -- #\n",
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "import nltk\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word\n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ],
   "id": "5af5f1cbe384a70f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:45:01.880119Z",
     "start_time": "2025-06-23T01:45:01.866073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 6.) Data Structuring -- #\n",
    "import torch # Import PyTorch library\n",
    "from torch.utils.data import TensorDataset, DataLoader # Import modules to create datasets and data loaders\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "input_ids = tokens['input_ids'] # Extract input IDs from the tokenized data\n",
    "attention_masks = tokens['attention_mask'] # Extract attention masks from the tokenized data\n",
    "\n",
    "# Define a mapping function\n",
    "def map_sentiment(value):\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity' is None\n",
    "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity'].tolist())"
   ],
   "id": "d7e794cdfe60c855",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T01:45:20.888466Z",
     "start_time": "2025-06-23T01:45:20.859405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -- 7.) Split The Dataset -- #\n",
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")"
   ],
   "id": "c7c3d186ab246f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and test sets are prepared with attention masks!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b3a8b4d59ccd9d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
